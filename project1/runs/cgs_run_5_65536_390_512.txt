Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 12 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
Traceback (most recent call last):
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 462 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 119 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 243 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 387 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 246 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 396 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 250 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 113 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 209 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 220 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 384 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 254 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 334 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 394 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 43 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 240 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 77 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 115 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 400 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 247 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 464 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 282 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 398 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 498 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 65 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 389 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 40 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 340 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 488 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 381 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 251 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 369 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 125 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 489 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 349 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 219 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 34 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 296 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 105 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 471 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 73 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 245 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 193 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 380 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 225 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 358 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 262 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 391 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 18 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 42 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 189 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 271 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
  File "/home/dinesh/MATH-505/project1/cgs.py", line 97, in <module>
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 495 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 152 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 463 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 442 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 169 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 141 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 227 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 88 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 297 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 107 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 263 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 477 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 61 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 327 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 278 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 481 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 505 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 422 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 144 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 24 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 405 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 435 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 207 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 162 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 134 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 183 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 32 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 39 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 38 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 244 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 44 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 242 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 35 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 252 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 47 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 249 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 37 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 255 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 36 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 248 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 41 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 203 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 45 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 241 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 196 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 371 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 33 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 199 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 197 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 383 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 202 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 379 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 198 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 378 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 201 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 372 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 206 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 374 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 195 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 382 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 200 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 368 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 192 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 375 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 204 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 373 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 377 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 167 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 205 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 175 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 172 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 376 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 164 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 160 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 143 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 170 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 131 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 165 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 132 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 174 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 138 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 161 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 128 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 171 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 129 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 163 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 140 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 168 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 133 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 166 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 137 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 130 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 178 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 136 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 185 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 139 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 181 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 135 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 186 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 191 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 179 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 232 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 180 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 226 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 184 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 228 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 187 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 235 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 176 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 230 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 177 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 229 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 188 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 231 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 234 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 86 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 182 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 224 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 233 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 83 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 239 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 82 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 236 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 91 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 238 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 85 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 89 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 95 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 347 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 80 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 342 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 87 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 350 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 81 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 337 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 92 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 344 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 336 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 295 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 338 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 298 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 345 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 93 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 343 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 94 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 348 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 84 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 346 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 293 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 96 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 341 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 290 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 351 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 302 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 292 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 103 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 303 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 111 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 289 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 106 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 294 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 301 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 299 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 98 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 300 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 366 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 354 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 288 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 360 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 357 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 256 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 352 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 364 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 261 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 359 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 361 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 269 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 365 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 266 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 362 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 260 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 367 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 267 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 356 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 258 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 363 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 265 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 353 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 257 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 268 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 264 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 270 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 116 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 7 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 112 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 126 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 117 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 114 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 123 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 124 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 9 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 120 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 127 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 121 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 469 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 118 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 475 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 465 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 466 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 15 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 479 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 473 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 472 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 474 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 4 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 476 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 14 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 467 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 470 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 468 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 11 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 305 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 13 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 310 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 307 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 5 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 311 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 10 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 309 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 304 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 2 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 319 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 3 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 308 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 8 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 314 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 315 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 316 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 318 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 78 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 312 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 79 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 317 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 313 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 72 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 64 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 70 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 52 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 76 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 57 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 66 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 59 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 71 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 48 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 75 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 215 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 68 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 67 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 69 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 222 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 223 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 321 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 214 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 328 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 221 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 333 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 208 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 331 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 212 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 329 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 217 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 323 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 210 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 330 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 216 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 325 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 213 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 335 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 324 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 218 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 326 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 320 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 482 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 487 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 274 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 286 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 485 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 332 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 277 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 275 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 494 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 272 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 483 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 273 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 486 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 281 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 279 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 490 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 285 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 491 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 280 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 480 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 284 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 484 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 287 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 492 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 276 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 501 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 496 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 499 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 504 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 399 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 502 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 390 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 507 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 511 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 393 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 510 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 392 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 500 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 388 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 506 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 386 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 508 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 395 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 509 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 385 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 503 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 423 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 159 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 431 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 147 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 420 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 156 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 426 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 157 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 418 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 150 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 419 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 149 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 424 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    Q_local, R = parallel_cgs(A_local, comm, matrix_rows, matrix_columns)
  File "/home/dinesh/MATH-505/project1/cgs.py", line 57, in parallel_cgs
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 425 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 145 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 430 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 146 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 417 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 151 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 421 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 154 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 428 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 148 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 416 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 427 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 155 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 20 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 454 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 457 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 21 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 450 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 28 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 452 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 22 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 456 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 30 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 448 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 26 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 458 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 27 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 455 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 16 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 451 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 29 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 158 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 461 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 453 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 31 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 459 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 17 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 460 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 23 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 25 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 408 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
    comm.gatherv(local_Q[:, :j], tst, root=0)
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 412 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 447 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 415 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 441 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 407 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
AttributeError: 'mpi4py.MPI.Intracomm' object has no attribute 'gatherv'. Did you mean: 'Gatherv'?
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 409 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 413 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 406 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 434 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 414 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 403 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 410 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 411 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 404 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 402 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 6 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
slurmstepd: error: *** STEP 4087021.0 ON h193 CANCELLED AT 2024-10-19T19:43:27 ***
srun: error: h211: tasks 288-303: Killed
srun: Terminating StepId=4087021.0
srun: error: h195: tasks 32-47: Killed
srun: error: h202: tasks 144-159: Killed
srun: error: h194: tasks 16-31: Killed
srun: error: h272: tasks 400-415: Killed
srun: error: h200: tasks 112-127: Killed
srun: error: h203: tasks 160-175: Killed
srun: error: h277: tasks 480-495: Killed
srun: error: h271: tasks 384-399: Killed
srun: error: h201: tasks 128-143: Killed
srun: error: h193: task 0: Exited with exit code 1
srun: error: h204: tasks 176-191: Killed
srun: error: h210: tasks 272-287: Killed
srun: error: h274: tasks 432-447: Killed
srun: error: h276: tasks 464-479: Killed
srun: error: h193: tasks 1-15: Killed
srun: error: h207: tasks 224-239: Killed
srun: error: h212: tasks 304-319: Killed
srun: error: h196: tasks 48-63: Killed
srun: error: h209: tasks 256-271: Killed
srun: error: h205: tasks 192-207: Killed
srun: error: h215: tasks 352-367: Killed
srun: error: h214: tasks 336-351: Killed
srun: error: h197: tasks 64-79: Killed
srun: error: h275: tasks 448-463: Killed
srun: error: h198: tasks 80-95: Killed
srun: error: h216: tasks 368-383: Killed
srun: error: h273: tasks 416-431: Killed
srun: error: h206: tasks 208-223: Killed
srun: error: h208: tasks 240-255: Killed
srun: error: h199: tasks 96-111: Killed
srun: error: h278: tasks 496-511: Killed
srun: error: h213: tasks 320-335: Killed
